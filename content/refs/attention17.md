---
layout: reference
title: Attention is All You Need
date: '2017'
authors:
  - Ashish Vaswani
  - Noam Shazeer
  - Niki Parmar
  - Jakob Uszkoreit
  - Llion Jones
  - Aidan N. Gomez
  - Lukasz Kaiser
  - Illia Polosukhin
journal: IEEE Transactions on Information Theory, Volume 22, Issue 6, November 1976, pp 644â€“654
links:
  - name: Google Research
    url: https://research.google/pubs/attention-is-all-you-need/
  - name: ArXiv
    url: https://arxiv.org/abs/1706.03762
  - name: PDF
    url: https://arxiv.org/pdf/1706.03762.pdf
  - name: Local PDF
    url: /files/papers/attention17.pdf
classic: data-science
---
Introduces the Transformer, a novel architecture that departs from the previous models reliant on recurrent layers. The core innovation is the attention mechanism that processes data in parallel and captures complex dependencies in sequences. This model significantly improves the efficiency of training models on tasks involving sequences, such as natural language processing (NLP) and machine translation. The Transformer has since become the foundation for subsequent advancements in NLP, including models like BERT and GPT, revolutionizing how machines understand and generate human language.
