---
layout: reference
title: Learning representations by back-propagating errors
date: '1986-10-09'
authors:
  - David E. Rumelhart
  - Geoffrey E. Hinton
  - Ronald J. Williams
links:
  - name: Article page at Nature
    url: https://www.nature.com/articles/323533a0
  - name: PDF
    url: http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf
classic: data-science
---
A foundational work in the field of neural networks. It introduces the concept of backpropagation, a method for efficiently training multi-layer neural networks through the adjustment of weights based on the gradient of the error function. This technique allows networks to learn complex representations of data by minimizing errors in a systematic way. Backpropagation has become a central algorithm in deep learning, enabling significant advancements in areas such as image recognition, natural language processing, and beyond.
